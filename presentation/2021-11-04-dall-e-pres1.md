---

slideOptions:
  transition: fade
  theme: white
  
---

## 1. Réseaux de neurones 

----

### 1.1 Du perceptron au perceptron multi-couche

----

#### Le perceptron : définition

* un neurone est une entité qui prend des signaux en entrée et qui renvoit un signal en sortie
* avec n entrées, est une fonction de $R^n*R^n*R$ dans $R$  $f(X,w,b)= \sigma(w'X+b)$
    * $X$ les signaux en entrée dans $R^n$
    * $w$ le vecteur des poids accordés à chaque signaux
    * $b$ le biais d'activation du neurone
    * $\sigma$ une fonction continue de $R$ dans $R$

----

#### Le perceptron : intérêt

* un neurone peut être entrainé (choix de $w$,$b$) pour apprendre un comportement (apprentissage supervisé)

----

#### Le perceptron : représentation graphique


* **source : https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf**
![perceptron](https://miro.medium.com/max/1302/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg)

----

#### Le perceptron : exemples

* prendre deux booléens et donner l'union
* prendre une image 1000*1000 pixels et dire si oui ou non c'est un chien
* prendre des mots et savoir si c'est écrit en français ou non
* besoin de classification 1= oui 0= a priori non, autre= ?
* e.g. pour les signaux, simplification dans $[0,1]$ avec comme but pour le signal de sortie d'être dans $\{0,1\}$

----

#### Le perceptron : Union

* prendre deux booléens et donner l'union
    *  $X1=0, X2=0 => Y_{expected}=0$
    *  $X1=0, X2=1 => Y_{expected}=1$
    *  $X1=1, X2=0 => Y_{expected}=1$
    *  $X1=1, X2=1 => Y_{expected}=1$
*  $w1=w2=1; b=1/2 ;\sigma=\mathbb{1}(wX-b>0)$
*  ou $w1=w2=10; b=5 ;\sigma=\mathbb{1}(wX-b>0)$

----

#### Le perceptron : Recap et limites

* entrée $X$ donnée, $(w , b)$ paramètres du neurone, $\sigma$ fonction d'activation choisie
* on peut trouver $(w,b)$ pour obtenir $Y$ à partir de $X$  pour des données linéairement séparables
* problématiques de l'overfitting et de l'interprétabilité ?

----

#### L'union fait la force : le perceptron multicouche

* les autres exemples énoncés précédemment demandent à combiner plusieurs neurones en couches pour être plus précis = Perceptron multi-couches
* **level up** : peut également prédire une variable catégorielle (binaire -> valeurs discrètes finies) =  nombre de neurones en couche de sortie

----

#### L'union fait la force : le perceptron multicouche

* possibilité de mettre plusieurs couches de neurones intermédiaires = gain ?
* **théorème d'approximation universelle** : un réseau de neurones avec au moins une couche intermédiaire peut approximer n'importe quelle fonction sur un compact de $R^n$ ( linéairement séparables => données quelconques)


----

#### Le perceptron multicouche : représentation graphique


* **source : https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf**
![MLP](https://miro.medium.com/max/1000/1*v1ohAG82xmU6WGsG2hoE8g.png)


----

#### Le perceptron multicouche : exemples 

* reconnaître l'écriture d'un chiffre et donner son chiffre
* régression logistique dans le cas non linéaire

----

#### Le perceptron multicouche : apprentissage

* fonction de coût/perte $L(W,B)$ : écart quadratique moyenne, valeur absolue moyenne
* but = **minimiser les erreurs** => apprendre => corriger les poids/biais
* $min_{W,B} L(W,B)$ 
* algorithme de descente du gradient

----

#### Le perceptron multicouche : descente du gradient

* calcul du gradient sur la fonction de perte
* correction des poids et biais des neurones
* **minimas locaux**

----

#### Le perceptron multicouche : limites

* dans le cas d'une image, prend les pixels en entrée de manière indépendante. Certains pixels dépendent de ses voisins (spatialement ou temporellement)
* sens du signal unidirectionnel

----

### 1.2 Vers des modèles avancées et l'au-delà

* CNN : Réseau de neurones entièrement connectés / Convolution / Déconvolution
* Réseau de neurones récurrents

----

#### Réseau de neurones convolutifs : but

* But : Réduction de dimensions - passer de $n*m$ dimensions à $d$ dimensions  ($d << n*m$)
* en rajoutant la notion de dépendance spatio-temporelle entre neurones

----

#### Réseau de neurones convolutifs : opérations

* Opération de convolution : ajouter l'information des voisins dans le neurone (ajouter de la dépendance)
* Opération de pooling : regrouper et réduire la dimension (perte d'informations redondantes ou non)
* Opération de correction
* Connexion à un réseau de neurones entièrement connectés

----

#### Réseau de neurones convolutifs : convolution

* **source : https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53**
![convolution](https://miro.medium.com/max/500/1*GcI7G-JLAQiEoCON7xFbhg.gif)

----

#### Réseau de neurones convolutifs : pooling

* **source : https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53**
![pooling](https://miro.medium.com/max/500/1*KQIEqhxzICU7thjaQBfPBQ.png)

----

#### Déconvolution

* But : reconstituer l'information en dimension $n*m$ à partir de l'information à $d$ dimensions
* il y a eu de la perte => reconstituer parfaitement n'est pas possible

----

#### Déconvolution : méthode

* Il suffit d'appliquer les opérations transposées de la convolution
* jeux sur les paramètres de striding et de padding
    * [A guide to convolution arithmetic for deep learning, Vincent Dumoulin, Francesco Visin
](https://arxiv.org/pdf/1603.07285.pdf)

----

#### Réseaux de neurones récurrents : Définition

* possibilité d'avoir des cycles dans le graphe des neurones
* but : travailler avec une mémoire, un contexte

----

#### Réseaux de neurones récurrents : représentation graphique

* **source : https://towardsdatascience.com/four-common-types-of-neural-network-layers-c0d3bb2a966c**
![RNN](https://miro.medium.com/max/333/1*evR7fkBJLxYD4mcbB-4YTw.png)
